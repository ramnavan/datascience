{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_datascience": {}
   },
   "source": [
    "This notebook contains code examples to connect to AWS S3, Redshift, MySQL, Hive, and SQL Server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_datascience": {}
   },
   "source": [
    "### S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_datascience": {},
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!pip install pandas==0.19 # for boto compatibility\n",
    "!pip install boto3\n",
    "import boto3\n",
    "def get_s3_client(access_key, secret_key):\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    Initialize s3 connection client. Run before any of the other s3 utility functions\n",
    "    \n",
    "    \"\"\"\n",
    "    s3_client = boto3.client(service_name = 's3', \n",
    "                      aws_access_key_id = access_key, \n",
    "                      aws_secret_access_key = secret_key)\n",
    "    return(s3_client)\n",
    "\n",
    "def s3_ls(s3_client, s3_bucket, path):\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    List contents of s3 directory specified in 'path'\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    if path != '' and path[-1] != '/':\n",
    "        path += '/'\n",
    "        \n",
    "    files = []\n",
    "    directories = []\n",
    "    \n",
    "    try:\n",
    "        for fname in s3_client.list_objects(Bucket = s3_bucket, Prefix = path)['Contents']:\n",
    "            \n",
    "            if '/' not in fname['Key'].replace(path,''):\n",
    "                files.append(fname['Key'].replace(path,''))\n",
    "            elif fname['Key'].replace(path,'').split('/')[0] + '/' not in directories:\n",
    "                directories.append(fname['Key'].replace(path,'').split('/')[0] + '/')\n",
    "                \n",
    "    except KeyError:\n",
    "        return('Directory Not Found')\n",
    "\n",
    "    return(directories + files)\n",
    "\n",
    "def s3_pull_file(s3_client, s3_bucket, s3_key, local_path = None):\n",
    "    \"\"\"\n",
    "    \n",
    "    Pull a file (any format) from S3 into the platform environment\n",
    "    Please supply the local_path without the file name (i.e. 'testing/test' NOT 'testing/test/test.csv')\n",
    "    \n",
    "    After the file has been pulled in, \n",
    "    it can be read into Python using the usual methods (e.g. open())\n",
    "    \n",
    "    If the local_path argument is not specified, the file will be placed in /home/jupyter/\n",
    "    \n",
    "    \"\"\"\n",
    "    # Create the directory to store the file on the platform\n",
    "    filename = s3_key.split('/')[-1]\n",
    "    if local_path is None:\n",
    "        local_dir = '/home/jupyter/'.format(filename)\n",
    "    else:\n",
    "        local_dir = '/home/jupyter/{0}'.format(local_path)\n",
    "        if not os.path.exists(local_dir):\n",
    "            os.makedirs(local_dir)\n",
    "        \n",
    "    # Download the file\n",
    "    s3_client.download_file(Bucket = s3_bucket, Key = s3_key, Filename = local_dir + '/' + filename)\n",
    "    \n",
    "    print(\"Your file is now available at '{0}'\".format(local_dir))\n",
    "    \n",
    "def s3_push_file(s3_client, s3_bucket, local_filepath, s3_filepath):\n",
    "    \"\"\"\n",
    "    \n",
    "    Push a file from the platform environment into S3\n",
    "    \n",
    "    \"\"\"\n",
    "    try:\n",
    "        s3_client.upload_file(local_filepath, s3_bucket, s3_filepath)\n",
    "        print(\"Uploaded to \" + s3_filepath)\n",
    "    except BaseException as e:\n",
    "        print(\"Upload error for \" + local_filepath)\n",
    "        print(str(e))\n",
    "        \n",
    "# Example calls\n",
    "s3_bucket = 'example_bucket'\n",
    "\n",
    "s3_client = get_s3_client(access_key = 'example_access_key'\n",
    "                           , secret_key = 'example_secret_key')\n",
    "\n",
    "s3_ls(s3_client = s3_client\n",
    "      , s3_bucket = s3_bucket\n",
    "      , path = 'example/key')\n",
    "\n",
    "s3_pull_file(s3_client = s3_client\n",
    "             , s3_bucket = s3_bucket\n",
    "             , s3_key = 'example/key/my.csv'\n",
    "             , local_path = 'local_example/test')\n",
    "\n",
    "s3_push_file(s3_client = s3_client\n",
    "             , s3_bucket = s3_bucket\n",
    "             , local_filepath = 'example/path.txt'\n",
    "             , s3_filepath = 'example/key')\n",
    "\n",
    "# Saves a pandas dataframe as a csv in S3\n",
    "def s3_DFtoCSV(dataframe, file_name):\n",
    "    \"\"\"\n",
    "    \n",
    "    Saves a pandas dataframe as a csv\n",
    "    \n",
    "    \"\"\"   \n",
    "    \n",
    "    s3, bucket = get_s3_client()\n",
    "    \n",
    "    str_csv=dataframe.to_csv(None, sep='\\t',index=False)\n",
    "    s3.put_object(Body=str_csv, Bucket=bucket, Key=file_name) \n",
    "\n",
    "# Loads a CSV into a pandas dataframe\n",
    "def s3_CSVtoDF(file_name, **kwargs):\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    Stream a data file from S3 into a dataframe\n",
    "    \n",
    "    All **kwargs are passed to pandas.read_csv() and must\n",
    "    therefore be valid keyword arguments of that function\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    s3, bucket = get_s3_client()\n",
    "    obj = s3.get_object(Bucket=bucket, Key=file_name)\n",
    "    \n",
    "    return pd.read_csv(obj['Body'], **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_datascience": {}
   },
   "source": [
    "### Redshift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_datascience": {},
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import pandas as pd\n",
    "\n",
    "conn = psycopg2.connect(\n",
    "    database = 'example_dbname',\n",
    "    port = 'example_port',\n",
    "    password = 'example_pw',\n",
    "    user = 'example_user',\n",
    "    host = 'example_host'\n",
    ")\n",
    "\n",
    "def results(query):\n",
    "    return pd.read_sql(query, conn)\n",
    "\n",
    "example_query = '''\n",
    "example query;\n",
    "'''\n",
    "\n",
    "data = results(example_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_datascience": {}
   },
   "source": [
    "### MySQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_datascience": {},
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!sudo apt-get -y update  # Can run this in a terminal \n",
    "!sudo apt-get -y install libmysqlclient-dev # Can run this in a terminal or include in requirements\n",
    "\n",
    "!pip install MySQL-python # can include in requirements file\n",
    "!pip install sqlalchemy # can include in requirements file\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "import MySQLdb as mdb\n",
    "\n",
    "conn = mdb.connect(os.environ['HOSTSQL'], os.environ['USERSQL'], os.environ['PASSWDSQL'], os.environ['DBSQL'])\n",
    "cur = conn.cursor()\n",
    "cur.execute('''\n",
    "example_statement;\n",
    "'''.format(dbsql, table_name))\n",
    "\n",
    "# To append data from a pandas dataframe onto an existing table\n",
    "engine = create_engine('mysql+mysqldb://{0}:{1}@{2}/{3}'.format(os.environ['USERSQL'],  os.environ['PASSWDSQL'], os.environ['HOSTSQL'], os.environ['DBSQL']), echo = False)\n",
    "result.to_sql(name = '{}'.format(table_name), con = engine, if_exists = 'append', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_datascience": {}
   },
   "source": [
    "### Hive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_datascience": {},
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import jaydebeapi\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import pandas as pd\n",
    "import jpype\n",
    "\n",
    "jvmPath = jpype.getDefaultJVMPath()\n",
    "jpype.startJVM(jvmPath,  \"-Djava.security.auth.login.config=/home/jupyter/jaas.conf\", \"-Djava.security.krb5.conf=/etc/krb5.conf\", \"-Djavax.security.auth.useSubjectCredsOnly=false\", \"-Djava.ext.dirs=/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/ext:/usr/lib/hive/lib:/usr/lib/hadoop:/usr/lib/hadoop/lib\", \"-Dlog4j.configuration=file:///etc/hive/conf/hive-log4j.properties\", \"-Dsun.security.krb5.debug=true\")\n",
    "conn = jaydebeapi.connect('org.apache.hive.jdbc.HiveDriver', 'example_address', [\"\",\"\"])\n",
    "df1 = pd.read_sql(\"example_query\", conn)\n",
    "print(df1)\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_datascience": {},
    "collapsed": true
   },
   "source": [
    "### SQL Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_datascience": {},
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pymssql\n",
    "\n",
    "server = getenv(\"PYMSSQL_TEST_SERVER\")\n",
    "user = getenv(\"PYMSSQL_TEST_USERNAME\")\n",
    "password = getenv(\"PYMSSQL_TEST_PASSWORD\")\n",
    "\n",
    "conn = pymssql.connect(server, user, password, \"tempdb\")"
   ]
  }
 ],
 "metadata": {
  "_datascience": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
